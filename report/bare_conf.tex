
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{./figures/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

\usepackage{adjustbox}

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Monolingual Written Natural Language Identification with Naïve Bayes and LSTMs}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Jacob Lundberg}
\IEEEauthorblockA{Linköping University\\
LiU-id: jaclu010\\
Email: jaclu010@student.liu.se}}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\IEEEauthorblockN{James Kirk\\ and Montgomery Scott}
%\IEEEauthorblockA{Starfleet Academy\\
%San Francisco, California 96678--2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
  This project aims to correctly classify the language of written text paragraphs. Given a paragraph from the WiLi dataset, one out of 235 languages can be classified. Chosen model solutions to the classification problem is a Naïve Bayes model and an LSTM. They reach a classification accuracy of 93.87\% and 63.87\% respectively. The Naïve Bayes model performs a lot better than the author of the WiLi dataset's neural network model, which achieves an accuracy of 88\%.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
\ifCLASSOPTIONpeerreview
\begin{center} \bfseries EDICS Category: 3-BBND \end{center}
\fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
The art of identifying the language in a text has been around a long time. It is something we humans do on a daily basis. The agent in question needs to understand what language is being read to interpret it correctly. It is an essential part of the pipeline of understanding text. 

In this project, the Wi-Li dataset~\cite{wili} is evaluated. The dataset in question contains phrases from Wikipedia in 235 different languages with a thousand (1000) paragraphs from each language.

As described by Thoma~\cite{wili}, a Multi-Layer Perceptron with character-based tf-idf features is implemented and achieves an overall accuracy of 88\%. Also, many existing classifiers are tested against said dataset but only reach a prediction accuracy of 22-36\%. The lousy performance is due to these classifiers only being implemented for a subset of languages existing in this dataset. The lack of existing solutions to both suggested classifiers by Thoma and other state-of-the-art models (to the WiLi dataset) makes for an exciting project. Thoma specifically states that models such as Naïve Bayes and recurrent neural networks (RNN) should be investigated further.

As mentioned by Mathur et. al. \cite{lide}, Naive
Bayes, SVM, n-gram, graph-based n-gram and,
prediction partial matching (PPM) are models that have been used for the task of language identification. Neural networks, specifically long short-term memory (LSTM) models in conjunction with word embeddings is currently popular in various natural language processing (NLP) tasks \cite{twitter, boundary, intent}. Also, Mathur et. al. reports success in the use of RNNs for the task of language identification \cite{lide}.

The idea of this project is to use a Naïve Bayes model as a baseline and then see if improvement is made with an advanced model, an LSTM neural network. The proposed LSTM consists of sequences of character n-grams.

% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

%\hfill jl
 
%\hfill January 10, 2019

\section{Theory}
This chapter will briefly describe the necessary theory, i.e. the theory behind the models used.

\subsection{tf-idf}
tf-idf is a common feature selection technique in NLP tasks. It is a short for \textit{term frequency - inverse document frequency} and it is a model to evaluate how relevant a word is given a set of documents.

Given a collection of $N$ documents, the frequency $f$ of word $i$ in document $j$ helps define the \textit{term frequency} $tf$ as 

$$tf_{i,j}= \frac{f_{i,j}}{\textnormal{max}_k f_{k,j}} \textnormal{,}$$

where $\textnormal{max}_kf_{k,j}$ denotes the word in the document with the highest frequency. This means that the word $j$ is normalized on the maximum occurance of any word in the document.


Then, suppose word $i$ occures in $n_i$ out of the $N$ documents. Then, the \textit{inverse document frequency} is defined as.

$$idf_i = \log_2(\frac{N}{n_i})\textnormal{.}$$

Finally, the two numbers are multiplied,

$$ tf\textnormal{-}idf = tf_{i,j} \times idf_i$$ 

\noindent and the word with the highest score usually carry most information about a document. \cite{tfidf}

\subsection{Multinomial Naïve Bayes}
A Naïve Bayes model is a derivation of a graph structure organized in such a way that there is assumed independence between all covariates.

Given a class $C_k$ out of $k$ possible classes and a word $x_i \in \boldsymbol{X}$, where $\boldsymbol{X}$ is the corpus, the probability of a specific class is given by


$$p(C_k, \boldsymbol{X}) = p(C_k) \prod_{i=1}^n p(x_i | C_k) \textnormal{.}$$

\noindent This is derived from Baye's theorem and the independence assumption between all $x_i \in \boldsymbol{X}$. \cite{ai-book}

Given this formula, in the classification problem, one would have to compute the probability for each class and pick the class with the highest probability as a classification \cite{itir}.

\subsection{Word embeddings}
Neural networks cannot on their own understand a text in string format. The strings need to be transformed into numbers or vectors. \textit{fastText} is one such model and it is a part of a theory called \textit{word embeddings}. 

fastText produces embedding vectors by looking at each n-gram of a word with a size of 3-6 characters. Then, a vector representation is assigned to each n-gram. Finally, the sum of the n-gram vectors represents the final embedding for the word. \cite{fasttext}

An advantage of fastText compared with Google's \textit{word2vec} \cite{word2vec} is that fastText can produce embeddings on words not yet seen in the data while training, otherwise known as out-of-vocabulary words. The advantage is limited to that at least some n-gram of a word were present during training. \cite{fasttext}

As described in the original paper, Mikolov et. al. \cite{word2vec} uses 1.6 billion words to train a 300 unit long word vector.

\subsection{LSTM}
An LSTM is one of many types of RNNs, specifically \textit{gated RNNs}. It is based on an idea that there can be paths throughout a sequence of inputs. This idea is closely related to another feature within the field called \textit{leaky units}. These paths carry information that may be accumulated through the sequence. As an opposite, it also means that some time information should be forgotten, which is also a feature within gated networks. \cite{dl-book}

\begin{figure}[h]
  \centering
      \includegraphics[scale=0.35]{LSTM}
        \caption{Layout of an LSTM cell}
        \label{fig:lstm}      
\end{figure}

The remainder of this section is a summary of previous work by Wang and Raj \cite{nnorigin}. Figure \ref{fig:lstm} shows a layout of a simple LSTM cell. One cell is responsible for one of the sequential inputs. 

\subsubsection{States}
States can be described as values passed between cells. In particular, the cell state $c_t$ is the memory of the cell (\textit{a)} in figure \ref{fig:lstm}) and can be passed on to other cells and layers. The hidden state $h_t$ can also be passed between cells and layers.

The input state is the linear combination of the previous hidden state and the input,
called $i_t$:

$$i_t=\sigma(W_{ix}x_t+W_{ih}h_{t-1})\textnormal{,}$$

where $W_{ab}$ are the corresponding weights and $\sigma$ denotes the \textit{sigmoid} function.

\subsubsection{Gates}
Gates in general uses the \textit{sigmoid} function to prune information or let it pass through a certain spot, since the function assumes a value on $[0,1]$ and the gate is multiplied to the corresponding spot. 

The \textit{forget gate}, $f_t$ (\textit{b)}), decides wether or not to keep the cell state from the previous time step. It is denoted with

$$f_t=\sigma(W_{fi}i_t)\textnormal{.}$$

The \textit{input gate}, $g_t$ (\textit{c)}), decides if the input state should be fed into the cell state. It is denoted by

$$g_t=\sigma(W_{gi}i_t)\textnormal{.}$$

Finally, there is the \textit{output gate}, $o_t$ (\textit{d)}), which decides if the current state is passed on as hidden state to the next cell,

$$o_t=\sigma(W_{oi}i_t)\textnormal{.}$$

These gates decide the flow of information, the final formula for the cell state is

$$c_t = g_t \odot i_t + f_t c_{t-1}\textnormal{,}$$

where $\odot$ denotes the element wise product. The hidden state is updated by

$$h_t = o_t \odot c_t\textnormal{.}$$

A common issue with RNNs is the vanishing/exploding gradient problem \cite{dl-book}. As gradients are backpropagated through the network, they tend to vanish or explode. In a lot of places, 3 times in figure \ref{fig:lstm}, values are multiplied. If the gradient then is continuously multipled with values smaller than one, the gradient will soon enough approach zero. The opposite is also the case when the gradient is continuously multiplied with a value larger than one.

\section{Data}
This chapter references Thoma \cite{wili}. As said, the dataset consists of 235 languages with text extracted from Wikipedia. Each language consists of 1000 paragraphs of text, making it a total of 235 000 paragraphs. The paragraphs were generated from random Wikipedia pages that consist of at least 140 characters in Unicode format. The dataset is already split into train and test data with 117 500 paragraphs each. Each of these sets are combined with labels of the language in pair with each paragraph.  The dataset is available online\footnote{https://doi.org/10.5281/zenodo.841984}.

\subsection{Balancing}
Even though each language contains a balanced amount of paragraphs, it does not mean that the paragraphs themselves are balanced. As an example, the shortest paragraph is 140 Unicode points long while the longest is 195 402 Unicode points long. The average length is 371 Unicode points.

\subsection{Issues in the dataset}
Thoma presents 5 distinct error sources within the data. The most disturbing are presented below.

\subsubsection{References} \label{E1}
Sometimes, books or other authors are mentioned in the article in question and is often written in the original language, which often is English. A concrete example is the second paragraph from the training set:

\textit{Sebes, Joseph; Pereira Thomas (1961) (på eng). The Jesuits and the Sino-Russian treaty of Nerchinsk (1689): the diary of Thomas Pereira. Bibliotheca Instituti historici S. I., 99-0105377-3 ; 18. Rome. Libris 677492}

The paragraph is from the Swedish Wikipedia, because of the string \textit{(på eng)}, which makes this paragraph hard to classify correct.

\subsubsection{Translation} \label{E2}
Some Wikipedia pages have english translations. A notable example is from the Tonganese Wikipedia \footnote{https://to.wikipedia.org/wiki/Sione\_Tupou\_Mateialona/en}.

\subsubsection{Quotes} \label{E3}
Many articles contain quotes, which are often written in the original language. This error is closely related to Section \ref{E1}.

%\subsubsection{Bias}
%Different authors have different writing styles. This also applies to the type of %article. A scientific article is expected to have different properties. 


\section{Method}
This chapter describes the steps taken to produce results. All implementation is done in Jupiter notebooks\footnote{https://jupyter.org/}, with Python 3\footnote{https://python.org/} as programming language. The resulting implementation is available online\footnote{https://github.com/jaclu010/text-mining}.

\subsection{Data collection}
The dataset consists of four \textit{.txt}-files. That is the train an test data along with their labels. On top of this, a sheet with more information regarding language shortenings was shipped with the data.

\subsection{Pre-processing}
The data consists of long paragraphs of text. As the data is not pre-processed in any way before, there still exists punctuation marks, spaces and other delimiters that may have an impact on the final performance. Therefore a regular expression is applied to each paragraph with the intent to remove all punctuation marks and to some extent correctly split words. 

Cleaning the data was harder than expected since Chinese, for example, uses another Unicode character for a comma in comparison to English. This and other examples based on primarily \cite{wikc, wikl, wikt} were added to the regular expression. This pre-processing was enough for the Naïve Bayes model. 

\subsubsection{LSTM pre-processing}
For the LSTM model, a fastText word embedding was trained on the data. The fastText model used a vector size of 100 with a 10 iteration training procedure. The fastText model was created with the \textit{Gensim}\footnote{https://radimrehurek.com/gensim/models/fasttext.html} implementation in Python. In total, 6 695 838 words were used to train the vector model.

Then, each word in a paragraph was concatenated with a blank space. After which all possible penta-grams of the paragraph in question were generated. Also, each paragraph was transformed into a sequence of 10 consecutive penta-grams. This made the dataset very large. To reduce the dataset, only 20\% of the training set and 10\% of the test set were used, chosen randomly with a uniform distribution. Also 10\% of the training data was used as validation data.

\subsection{Multinomial Naïve Bayes}
This model was implemented with \textit{sk-learn}'s\footnote{https://scikit-learn.org/} implementation of the Naïve Bayes model. First, a \textit{count vectorizer} was applied onto the training data, which counts occurrences of each word creating a 2D matrix with documents and counts.

Two models were created. One which only uses the previously mentioned count matrix (further referenced as \textit{NB-cv}) and the second which converts the count matrix to \textit{tf-idf} scores before training (further referenced as \textit{NB-tf-idf}).

The test set was then transformed into a count matrix which is used by both models to make predictions. The models kept an internal mapping to the language in question and were able to produce language labels. Thus, the data was ready for evaluation.

\subsection{LSTM}
The LSTM model was created with \textit{Keras}\footnote{https://keras.io/} with \textit{Tensorflow}\footnote{https://www.tensorflow.org/} as backend.

The model consists of three layers. One LSTM layer with 100 LSTM units, a dropout layer with a dropout fraction of 0.4 and finally a softmax output layer with 235 neurons, one for each language. For each input, the index of the softmax neuron with the highest score is the resulting prediction.

\subsection{Evaluation}
 Evaluation was done by calcuating accuracy, precision, recall and F1-score.

\section{Results}
This section presents the results obtained. They are shown as percentages in table \ref{tbl:results}. A detailed confusion matrix for the NB-cv model can be found in Appendix \ref{appendix}, figure \ref{fig:conf_mat_cv} and for the LSTM model in figure \ref{fig:conf_mat_lstm}. 

\begin{table}[h]
  \caption{Results for the classifiers, numbers in percent \%}
  \label{tbl:results}
  \centering
  \begin{tabular}{l|l|l|l}
    \textbf{\%}              & \textbf{NB-cv} & \textbf{NB-tf-idf} & \textbf{LSTM}                       \\ \hline
    \textbf{Accuracy}        & 93.87          & 92.99              & \multicolumn{1}{l|}{63.87}          \\ \hline
    \textbf{Macro precision} & \textbf{95.89} & \textbf{95.25}     & \multicolumn{1}{l|}{\textbf{69.27}} \\ \hline
    \textbf{Micro precision} & 93.87          & 92.99              & \multicolumn{1}{l|}{63.87}          \\ \hline
    \textbf{Recall}          & 93.87          & 92.99              & \multicolumn{1}{l|}{63.87}          \\ \hline
    \textbf{F1-score}        & 94.39          & 93.52              & \multicolumn{1}{l|}{59.34}         
    \end{tabular}
\end{table}

In table \ref{tbl:results}, regarding model NB-cv, it is noted that 2411 paragraphs are classified as English. The same phenomena is noted for the LSTM model where 2216 paragraphs are classified as English. Thus, a lot of paragraphs are wrongly classified and manual inspection shows indication of errors mentionend in Sections \ref{E1}, \ref{E2} and, \ref{E3}. 

The LSTM model performs a lot worse on all aspects. Also noted was the training set accuracy of 39.22\% and validation set accuracy of 46.30\%.


\section{Discussion} \label{discussion}

Naïve Bayes work extremely well in this setting. The NB-cv model (94\% accuracy) is notably better than Thoma's 88\% accuracy neural network. It is also notable that the model performs better when only the \textit{count vectorized} data is used, compared to \textit{tf-idf} transformed data. The performance increase can be explained by features that \textit{tf-idf} intend to scale down is of some use. A frequently appearing sequence of characters in English is \textit{the}. However, these three characters are also a good indication that the language in question \textit{is} English; hence it might answer the improved performance. Also, some languages have unique characters that only their language use, resulting in small to none overlap regarding prediction probabilities. Mathur et al. \cite{lide} also concludes this fact.

The LSTM model did not perform as well as expected. A lot of the times, overfitting the training data was a common problem. Despite numerous attempts to prevent overfitting, it happened already on the second epoch. Another sign of a poor fit is the difference in training and test set accuracy. A poor fit may be the result of numerous errors sources; the size of the word embedding vector; the type of n-gram used; not enough dropout in the model; the type of optimizer and, the layout of the model. Not only do the results disappoint but it also proves the difficulty of the classification task. Also, since the results of the models differ a lot, no discussion is made concerning the different metrics used.

One solution that Mathur et al. \cite{lide} proposes and uses is an ensemble of RNN classifiers with different type of n-grams, which, in their case improve performance. There are multiple ways of improving the model in this project. As an example, only penta-grams were used as a feature. As so, the model can be extended to multiple n-grams or different types of n-grams.

As previously stated, the original \textit{word2vec} uses 1.6 billion words to train a word vector. The model of this project uses a corpus that is 250 times smaller. At the same time, this relatively small corpus need to account for 235 languages. The small corpus is an issue that impacts performance, as it might introduce unwanted relationships between word vectors. 

Some languages have no word boundaries and the fact that one character can represent multiple words, which means that a regular space might not always be a good split between words. All models used in this project is based on space delimited words, and it probably has some impact. However, the NB-cv model still performs very well. A way to tackle this issue is to use byte-grams, where an entire paragraph is split into n-grams taking all delimiters into account.

The dataset shows multiple occurrences were the paragraphs contain mostly English words. It leaves room for errors as even a human could have misunderstood the language of a paragraph. A good example of this is already mentioned in Section \ref{E1}. Manual inspection of all paragraphs is a cumbersome project, but the error sources mentioned in Sections \ref{E1}, \ref{E2}, \ref{E3} and, described by Thoma \cite{wili} do contribute to a decrease in performance. However, in many circumstances today, multiple languages appear together. The added noise to the data proves that our models (at least NB-cv and NB-tf-idf) are very robust.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}
A simple model such as Naïve Bayes works extremely well in this setting. At this time the NB-cv model is, to my knowledge, the best performing model on the WiLi dataset, with an accuracy of 93.87\%.

The LSTM needs further development. Some are discussed in Section \ref{discussion}. This is an example when the time invested in developing an advanced neural network might not be worth it when the Naïve Bayes model performs extremely good.




% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{ref}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

\appendix%{Confusion matrices}
\section{Confusion matrices}
\label{appendix}
\begin{figure*}[h]
  \centering
      \includegraphics[width=\textwidth]{conf_mat_cv}
		\caption{Confusion matrix for the NB-cv model. The color bar shows prediction counts where black is zero and white is 500}
		\label{fig:conf_mat_cv}      
\end{figure*}
\begin{figure*}[h]
  \centering
      \includegraphics[width=\textwidth]{conf_mat_lstm}
		\caption{Confusion matrix for the NB-cv model. The color bar shows prediction counts where white is zero and black is 500}
		\label{fig:conf_mat_lstm}      
\end{figure*}
% that's all folks
\end{document}


