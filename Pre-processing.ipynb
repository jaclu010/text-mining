{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jacke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import FastText\n",
    "from nltk import word_tokenize\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    with open(fname, encoding=\"utf8\") as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    \n",
    "    return pds.DataFrame(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pds.concat([read_file('x_train.txt'), read_file('y_train.txt')], axis=1)\n",
    "train.columns = ['x', 'label']\n",
    "\n",
    "test = pds.concat([read_file('x_test.txt'), read_file('y_test.txt')], axis=1)\n",
    "test.columns = ['x', 'label']\n",
    "\n",
    "train['cat'] = train.label.factorize()[0]\n",
    "id_to_lang = train[['label', 'cat']].drop_duplicates().sort_values('cat').reset_index(drop=True)\n",
    "\n",
    "cats = []\n",
    "for idx, row in test.iterrows():\n",
    "    cats.append(id_to_lang[id_to_lang.label == row.label].cat.values[0])\n",
    "    \n",
    "test['cat'] = cats\n",
    "test.cat.to_pickle(\"y_test_original.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex for sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indtil 1545 havde flådecheferne kunnet hyre et betydeligt antal frie mænd til galejerne, selv om de kun sjældent var venetianere. De kom fra Dalmatien, Kreta og Grækenland. Herefter gik man i stigende grad over til tvangsudskrivning af fanger og skyldnere, ligesom det længe havde været normalt i resten af Europa. På langt sigt havde det den konsekvens for arbejdsmarkedet, at stadig færre lønmodtagere tjente deres penge på havet.'] \n",
      "----\n",
      "['Indtil', '1545', 'havde', 'flådecheferne', 'kunnet', 'hyre', 'et', 'betydeligt', 'antal', 'frie', 'mænd', 'til', 'galejerne', ',', 'selv', 'om', 'de', 'kun', 'sjældent', 'var', 'venetianere', '.', 'De', 'kom', 'fra', 'Dalmatien', ',', 'Kreta', 'og', 'Grækenland', '.', 'Herefter', 'gik', 'man', 'i', 'stigende', 'grad', 'over', 'til', 'tvangsudskrivning', 'af', 'fanger', 'og', 'skyldnere', ',', 'ligesom', 'det', 'længe', 'havde', 'været', 'normalt', 'i', 'resten', 'af', 'Europa', '.', 'På', 'langt', 'sigt', 'havde', 'det', 'den', 'konsekvens', 'for', 'arbejdsmarkedet', ',', 'at', 'stadig', 'færre', 'lønmodtagere', 'tjente', 'deres', 'penge', 'på', 'havet', '.'] \n",
      "----\n",
      "['Indtil', 'havde', 'flådecheferne', 'kunnet', 'hyre', 'et', 'betydeligt', 'antal', 'frie', 'mænd', 'til', 'galejerne', 'selv', 'om', 'de', 'kun', 'sjældent', 'var', 'venetianere', 'De', 'kom', 'fra', 'Dalmatien', 'Kreta', 'og', 'Grækenland', 'Herefter', 'gik', 'man', 'i', 'stigende', 'grad', 'over', 'til', 'tvangsudskrivning', 'af', 'fanger', 'og', 'skyldnere', 'ligesom', 'det', 'længe', 'havde', 'været', 'normalt', 'i', 'resten', 'af', 'Europa', 'På', 'langt', 'sigt', 'havde', 'det', 'den', 'konsekvens', 'for', 'arbejdsmarkedet', 'at', 'stadig', 'færre', 'lønmodtagere', 'tjente', 'deres', 'penge', 'på', 'havet', '']\n"
     ]
    }
   ],
   "source": [
    "# Regex testing cell\n",
    "x = train.iloc[52].x\n",
    "reg = re.compile(\"[,.、，() «»\\[\\]0-9-_\\!\\?—\\\"=&%#。‧《》〈〉 ໌﹏༌་\\s\\n\\r\\t \\xA0\\u1680\\u180E\\u2000-\\u200B\\u202F\\u205F\\u3000\\uFEFF]+\")\n",
    "print(x.split(\"，\"), \"\\n----\")\n",
    "print(word_tokenize(x), \"\\n----\")\n",
    "print(reg.split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg = re.compile(\"[,.、，;:() «»\\[\\]0-9-_\\!\\?—\\\"=&%#。‧《》〈〉 ໌﹏༌་\\s\\n\\r\\t \\xA0\\u1680\\u180E\\u2000-\\u200B\\u202F\\u205F\\u3000\\uFEFF]+\")\n",
    "corpus = [[w for w in reg.split(sent.x)] for idx, sent in train.iterrows()]\n",
    "corpus_y = [[w for w in reg.split(sent.x)] for idx, sent in test.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and count the words used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6695838\n"
     ]
    }
   ],
   "source": [
    "words = 0\n",
    "for c in corpus:\n",
    "    words += len(c)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate corpus and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(sent) for sent in corpus]\n",
    "ncorp = [\" \".join(row) for row in corpus]\n",
    "y_ncorp = [\" \".join(row) for row in corpus_y]\n",
    "\n",
    "pds.DataFrame(ncorp).to_pickle(\"train_data_regex.pkl\")\n",
    "pds.DataFrame(y_ncorp).to_pickle(\"test_data_regex.pkl\")\n",
    "id_to_lang.to_pickle('id_to_lang.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate n-gram dataset for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(file, grams, N, c, cats, prob):\n",
    "    g_len = len(grams)\n",
    "    lines_written = 0\n",
    "    for i in range(g_len):\n",
    "        if np.random.random() < prob:\n",
    "            for j in range(i,i+N):\n",
    "                if j >= g_len:\n",
    "                    file.write(\"\".join(grams[j%g_len]))\n",
    "                else:\n",
    "                    file.write(\"\".join(grams[j]))\n",
    "                if j != i+N-1:\n",
    "                    file.write(\"&\")\n",
    "            lines_written += 1\n",
    "            file.write(\"\\n\")\n",
    "            cats.append(c)\n",
    "        \n",
    "    return lines_written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the new datasets, they are in .txt format to be used by generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 5\n",
    "N = 10\n",
    "train_cats = []\n",
    "valid_cats = []\n",
    "msk = np.random.rand(len(ncorp)) < 0.9\n",
    "with open(\"x_train_data.txt\", \"w\", encoding=\"utf8\") as file, open(\"x_valid_data.txt\", \"w\", encoding=\"utf8\") as file2:\n",
    "    for i in range(len(ncorp)):\n",
    "        row = ncorp[i]\n",
    "        c = train.iloc[i]['cat']\n",
    "        grams = list(ngrams(row, n_gram, pad_left=True, \n",
    "                       left_pad_symbol=\" \",\n",
    "                       pad_right=True,\n",
    "                       right_pad_symbol=\" \"))\n",
    "        if msk[i]:\n",
    "            write_to_file(file, grams, N, c, train_cats, 0.3)\n",
    "        else:\n",
    "            write_to_file(file2, grams, N, c, valid_cats, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cats = []\n",
    "test_lens = []\n",
    "with open(\"x_test_data.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "    for i in range(len(y_ncorp)):\n",
    "        row = y_ncorp[i]\n",
    "        c = test.iloc[i]['cat']\n",
    "        grams = list(ngrams(row, n_gram, pad_left=True, \n",
    "                       left_pad_symbol=\" \",\n",
    "                       pad_right=True,\n",
    "                       right_pad_symbol=\" \"))\n",
    "        test_lens.append(write_to_file(file, grams, N, c, test_cats, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also save the length of all n-gram sequence belonging to one row in the test set, and the labels for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds.Series(test_lens).to_pickle('x_test_lens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_train_id.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "    for row in train_cats:\n",
    "        file.write(str(row) +\"\\n\")\n",
    "        \n",
    "with open(\"y_valid_id2.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "    for row in valid_cats:\n",
    "        file.write(str(row) + \"\\n\")\n",
    "        \n",
    "with open(\"y_test_id.txt\", \"w\", encoding=\"utf8\") as file:\n",
    "    for row in test_cats:\n",
    "        file.write(str(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To each (test set) n-gram sequence, also save the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = id_to_lang.to_dict('index')\n",
    "y_test_text_labels = [ids[test_cats[i]]['label'] for i in range(len(test_cats))]\n",
    "pds.Series(y_test_text_labels).to_pickle(\"y_test_labels.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv_size = 100\n",
    "ft_model = FastText(corpus, size = wv_size, workers = 8, iter= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ft_model.save(\"fasttext_5-gram.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
